"""
Data Loader ‚Äî –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å Bybit
–ü–µ—Ä–∏–æ–¥: 2023-01-01 –¥–æ 2025-12-31 (3 –≥–æ–¥–∞)
"""
import asyncio
import aiohttp
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict
import json

from app.core.logger import logger


class BybitDataLoader:
    """
    –ó–∞–≥—Ä—É–∑—á–∏–∫ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å Bybit API
    
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ —á–∞—Å—Ç—è–º (–ª–∏–º–∏—Ç 1000 —Å–≤–µ—á–µ–π –∑–∞ –∑–∞–ø—Ä–æ—Å)
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ CSV
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—Å–µ—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
    """
    
    BASE_URL = "https://api.bybit.com"
    DATA_DIR = Path("/root/crypto-bot/data")
    
    SYMBOLS = ["BTC", "ETH", "BNB", "SOL", "XRP", "ADA", "DOGE", "MATIC", "LINK", "AVAX"]
    
    # Bybit API intervals: 1, 3, 5, 15, 30, 60, 120, 240, 360, 720, D, W, M
    TIMEFRAMES = {
        "1m": ("1", 1),
        "5m": ("5", 5),
        "15m": ("15", 15),
        "30m": ("30", 30),
        "1h": ("60", 60),
        "4h": ("240", 240),
        "1d": ("D", 1440),
    }
    
    def __init__(self):
        self.DATA_DIR.mkdir(exist_ok=True)
        self.session: Optional[aiohttp.ClientSession] = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30)
        )
        return self
    
    async def __aexit__(self, *args):
        if self.session and not self.session.closed:
            await self.session.close()
            self.session = None
    
    async def _ensure_session(self):
        """–ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —á—Ç–æ —Å–µ—Å—Å–∏—è –∞–∫—Ç–∏–≤–Ω–∞"""
        if self.session is None or self.session.closed:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30)
            )
    
    def _get_cache_path(self, symbol: str, timeframe: str, start_year: int, end_year: int) -> Path:
        """–ü—É—Ç—å –∫ –∫—ç—à-—Ñ–∞–π–ª—É"""
        return self.DATA_DIR / f"{symbol}_{timeframe}_{start_year}_{end_year}.csv"
    
    async def _fetch_klines(
        self,
        symbol: str,
        interval: str,
        start_time: int,
        end_time: int,
        limit: int = 1000
    ) -> List[list]:
        """–ó–∞–ø—Ä–æ—Å —Å–≤–µ—á–µ–π —Å API"""
        
        url = f"{self.BASE_URL}/v5/market/kline"
        params = {
            "category": "spot",
            "symbol": f"{symbol}USDT",
            "interval": interval,
            "start": start_time,
            "end": end_time,
            "limit": limit
        }
        
        # –£–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ —Å–µ—Å—Å–∏—è –∞–∫—Ç–∏–≤–Ω–∞
        await self._ensure_session()
        
        try:
            async with self.session.get(url, params=params) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    if data.get("retCode") == 0:
                        return data.get("result", {}).get("list", [])
                    else:
                        logger.warning(f"API error: {data.get('retMsg')}")
        except asyncio.TimeoutError:
            logger.warning(f"Timeout fetching {symbol}")
        except Exception as e:
            logger.error(f"Error fetching {symbol}: {e}")
        
        return []
    
    async def download_symbol(
        self,
        symbol: str,
        timeframe: str = "5m",
        start_date: str = "2023-01-01",
        end_date: str = "2025-12-31",
        force: bool = False
    ) -> Optional[pd.DataFrame]:
        """
        –°–∫–∞—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        
        Args:
            symbol: BTC, ETH, etc.
            timeframe: 1m, 5m, 15m, 1h, 4h, 1d
            start_date: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞
            end_date: –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞
            force: –ü–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å –∫—ç—à
        """
        
        logger.info(f"üì• Downloading {symbol} {timeframe} from {start_date} to {end_date}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_file = self._get_cache_path(symbol, timeframe, int(start_date[:4]), int(end_date[:4]))
        
        if cache_file.exists() and not force:
            logger.info(f"üìÇ Loading from cache: {cache_file}")
            df = pd.read_csv(cache_file, parse_dates=['timestamp'])
            return df
        
        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º end_date —Ç–µ–∫—É—â–µ–π –¥–∞—Ç–æ–π
        now = datetime.utcnow()
        if end_dt > now:
            end_dt = now
            logger.info(f"  Adjusted end date to {end_dt.strftime('%Y-%m-%d')}")
        
        # –ò–Ω—Ç–µ—Ä–≤–∞–ª: (api_value, minutes)
        tf_data = self.TIMEFRAMES.get(timeframe, ("5", 5))
        api_interval = tf_data[0]
        interval_minutes = tf_data[1]
        
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        all_data = []
        current_end = end_dt
        
        # Bybit –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ (–æ—Ç –Ω–æ–≤—ã—Ö –∫ —Å—Ç–∞—Ä—ã–º)
        while current_end > start_dt:
            # –ó–∞–ø—Ä–æ—Å
            end_ts = int(current_end.timestamp() * 1000)
            
            klines = await self._fetch_klines(
                symbol=symbol,
                interval=api_interval,
                start_time=int(start_dt.timestamp() * 1000),
                end_time=end_ts,
                limit=1000
            )
            
            if klines:
                all_data.extend(klines)
                
                # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—É—é —Å—Ç–∞—Ä—É—é —Å–≤–µ—á—É –≤ —ç—Ç–æ–º —á–∞–Ω–∫–µ
                oldest_ts = min(int(k[0]) for k in klines)
                current_end = datetime.fromtimestamp(oldest_ts / 1000) - timedelta(minutes=1)
                
                logger.debug(f"  Got {len(klines)} candles, oldest: {current_end}")
            else:
                break
            
            # Rate limiting
            await asyncio.sleep(0.15)
        
        if not all_data:
            logger.error(f"‚ùå No data for {symbol}")
            return None
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame
        df = pd.DataFrame(all_data, columns=[
            'timestamp', 'open', 'high', 'low', 'close', 'volume', 'turnover'
        ])
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–∏–ø—ã
        df['timestamp'] = pd.to_datetime(df['timestamp'].astype(int), unit='ms')
        for col in ['open', 'high', 'low', 'close', 'volume']:
            df[col] = df[col].astype(float)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        df = df.drop_duplicates(subset=['timestamp'])
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–∞—Ç–∞–º
        df = df[(df['timestamp'] >= start_dt) & (df['timestamp'] <= end_dt)]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        df.to_csv(cache_file, index=False)
        logger.info(f"üíæ Saved {len(df):,} candles to {cache_file}")
        
        return df
    
    async def download_all_symbols(
        self,
        timeframe: str = "5m",
        start_date: str = "2023-01-01",
        end_date: str = "2025-12-31",
        force: bool = False
    ) -> Dict[str, pd.DataFrame]:
        """–°–∫–∞—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        
        results = {}
        
        for symbol in self.SYMBOLS:
            df = await self.download_symbol(
                symbol=symbol,
                timeframe=timeframe,
                start_date=start_date,
                end_date=end_date,
                force=force
            )
            
            if df is not None and len(df) > 0:
                results[symbol] = df
                logger.info(f"‚úÖ {symbol}: {len(df):,} candles")
            else:
                logger.warning(f"‚ö†Ô∏è {symbol}: No data")
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å–∏–º–≤–æ–ª–∞–º–∏
            await asyncio.sleep(0.5)
        
        return results
    
    def load_from_cache(self, symbol: str, timeframe: str = "5m") -> Optional[pd.DataFrame]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –∫—ç—à–∞"""
        
        # –ò—â–µ–º CSV —Ñ–∞–π–ª—ã –¥–ª—è —ç—Ç–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        pattern = f"{symbol}_{timeframe}_*.csv"
        files = list(self.DATA_DIR.glob(pattern))
        
        if files:
            # –ë–µ—Ä—ë–º —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π
            latest = max(files, key=lambda f: f.stat().st_mtime)
            logger.info(f"üìÇ Loading from CSV: {latest}")
            return pd.read_csv(latest, parse_dates=['timestamp'])
        
        # –ò—â–µ–º JSON —Ñ–∞–π–ª—ã –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞
        # –§–æ—Ä–º–∞—Ç: SYMBOL_YEAR_TF.json
        json_pattern = f"{symbol}_*_{timeframe}.json"
        json_files = list(self.DATA_DIR.glob(json_pattern))
        
        if not json_files:
            # –ü—Ä–æ–±—É–µ–º –±–µ–∑ timeframe
            json_files = list(self.DATA_DIR.glob(f"{symbol}_*.json"))
        
        if json_files:
            latest = max(json_files, key=lambda f: f.stat().st_mtime)
            logger.info(f"üìÇ Loading from JSON: {latest}")
            
            with open(latest, 'r') as f:
                data = json.load(f)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã JSON
            if isinstance(data, dict):
                # –§–æ—Ä–º–∞—Ç: {"klines": [...], "symbol": "BTC", ...}
                if 'klines' in data:
                    df = pd.DataFrame(data['klines'])
                else:
                    # –§–æ—Ä–º–∞—Ç: {"timestamp": [...], "open": [...], ...}
                    df = pd.DataFrame(data)
            elif isinstance(data, list):
                # –§–æ—Ä–º–∞—Ç: [{...}, {...}, ...]
                df = pd.DataFrame(data)
            else:
                logger.error(f"Unknown JSON format in {latest}")
                return None
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp
            if 'timestamp' in df.columns:
                # –ï—Å–ª–∏ timestamp –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö (–±–æ–ª—å—à–æ–µ —á–∏—Å–ª–æ)
                if df['timestamp'].iloc[0] > 1e12:
                    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
                else:
                    df['timestamp'] = pd.to_datetime(df['timestamp'])
            elif 'datetime' in df.columns:
                df['timestamp'] = pd.to_datetime(df['datetime'])
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –µ—Å—Ç—å –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            required = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
            if all(col in df.columns for col in required):
                return df[required].sort_values('timestamp').reset_index(drop=True)
            else:
                logger.warning(f"Missing columns. Available: {df.columns.tolist()}")
                return df
        
        return None
    
    def get_available_data(self) -> Dict[str, List[str]]:
        """–ü–æ–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        available = {}
        
        for symbol in self.SYMBOLS:
            files = list(self.DATA_DIR.glob(f"{symbol}_*.csv"))
            if files:
                available[symbol] = [f.name for f in files]
        
        return available


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
data_loader = BybitDataLoader()


async def download_all_data():
    """–°–∫—Ä–∏–ø—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    logger.info("üöÄ Starting data download for 2023-2025...")
    
    async with BybitDataLoader() as loader:
        results = await loader.download_all_symbols(
            timeframe="5m",
            start_date="2023-01-01",
            end_date="2025-12-31"
        )
    
    logger.info(f"‚úÖ Downloaded data for {len(results)} symbols")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_candles = sum(len(df) for df in results.values())
    logger.info(f"üìä Total candles: {total_candles:,}")
    
    return results


if __name__ == "__main__":
    asyncio.run(download_all_data())
